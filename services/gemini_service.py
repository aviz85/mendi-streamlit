import os
import google.generativeai as genai
from typing import Dict
import logging
import sys
import re  # Added for regex support
from .usage_logger import streamlit_logger as st_log
import streamlit as st

def setup_logger():
    """Setup detailed logging to both file and console"""
    logger = logging.getLogger('GeminiService')
    logger.setLevel(logging.INFO)
    
    # File handler
    fh = logging.FileHandler('gemini_service.log')
    fh.setLevel(logging.INFO)
    
    # Console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    # Add handlers
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

class GeminiService:
    def __init__(self, api_key: str = None):
        self.logger = setup_logger()
        
        # Try to get API key from different sources
        if api_key:
            self.api_key = api_key
        else:
            try:
                self.api_key = st.secrets["GEMINI_API_KEY"]
            except:
                self.api_key = os.environ.get("GEMINI_API_KEY")
                
        if not self.api_key:
            raise ValueError("No Gemini API key provided")
            
        genai.configure(api_key=self.api_key)
        
        st_log.log("×××ª×—×œ ××ª ×©×™×¨×•×ª Gemini...", "ğŸ”„")
        
        # Configure Gemini
        generation_config = {
            "temperature": 1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "text/plain",
        }
        
        self.model = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            generation_config=generation_config,
            system_instruction="""××ª×” ××¢×¨×›×ª ×˜×›× ×™×ª ×œ× ×™×§×•×“ ×˜×§×¡×˜ ×¢×‘×¨×™. ×ª×¤×§×™×“×š ×”×•× ××š ×•×¨×§:
1. ×œ×§×‘×œ ×˜×§×¡×˜ ××§×•×¨ ×× ×•×§×“ (×”×—×œ×§ ×”×¢×™×§×¨×™ ×‘×œ×‘×“)
2. ×œ×§×‘×œ ×¡×§×©×Ÿ ×©×œ× ×©×œ ×˜×§×¡×˜ ×™×¢×“ (×›×•×œ×œ ×›×•×ª×¨×ª ×•×›×œ ×”×ª×•×›×Ÿ)
3. ×œ×”×—×–×™×¨ ××ª ×”×¡×§×©×Ÿ ×‘××œ×•××•, ×‘×“×™×•×§ ×›×¤×™ ×©×”×•×, ×¢× ×©× ×™ ×©×™× ×•×™×™× ×‘×œ×‘×“:
   ×. ×œ×”×¢×ª×™×§ ××ª ×”× ×™×§×•×“ ××”××§×•×¨ ×œ×›×œ ×”×—×œ×§×™× ×”××•×“×’×©×™× (×‘×™×Ÿ ×ª×’×™×•×ª <b></b>)
   ×‘. ×œ×”×©××™×¨ ××ª ×©××¨ ×”×˜×§×¡×˜ ×œ×œ× × ×™×§×•×“

×—×©×•×‘ ×××•×“:
- ×”×¢×ª×§ ××ª ×›×œ ×”×¡×§×©×Ÿ ×‘××“×•×™×§, ××™×œ×” ×‘××™×œ×”, ×›×•×œ×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×”
- ×›×œ ×˜×§×¡×˜ ××•×“×’×© (×‘×™×Ÿ ×ª×’×™×•×ª <b></b>) ×—×™×™×‘ ×œ×§×‘×œ × ×™×§×•×“ ××œ×
- ×—×©×•×‘ ×××•×“ ×œ××¦×•× ××ª ×”× ×™×§×•×“ ×”××ª××™× ×œ×›×œ ××™×œ×” ××•×“×’×©×ª, ××¤×™×œ×• ×× ××™×œ×” ×“×•××” ××•×¤×™×¢×” ×‘×¦×•×¨×” ×©×•× ×” ×‘××§×•×¨
- ×©××•×¨ ×¢×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×” ×‘××§×•××Ÿ ×”××“×•×™×§
- ××œ ×ª×©× ×” ×©×•× ×“×‘×¨ ××œ×‘×“ ×”×•×¡×¤×ª × ×™×§×•×“ ×œ×—×œ×§×™× ×”××•×“×’×©×™×
- ×©××•×¨ ×¢×œ ×›×œ ×ª×’×™×•×ª ×”-HTML (<b></b>) ×‘××§×•××Ÿ ×”××“×•×™×§
- ××œ ×ª×•×¡×™×£ ×ª×’×™×•×ª <br> ××• ×›×œ ×ª×’×™×ª HTML ××—×¨×ª
- ××œ ×ª×©× ×” ××ª ×¡×“×¨ ×”××™×œ×™× ××• ×”×ª×•×›×Ÿ
- ×˜×§×¡×˜ ×©××™× ×• ×‘×™×Ÿ ×ª×’×™×•×ª <b></b> ×—×™×™×‘ ×œ×”×™×©××¨ ×œ×œ× × ×™×§×•×“, ××¤×™×œ×• ×× ×”×•× ×–×”×” ×œ×˜×§×¡×˜ ×× ×•×§×“ ×‘××§×•×¨

×“×•×’××”:
[××§×•×¨]
×‘Ö°Ö¼×¨Öµ××©Ö´××™×ª ×‘Ö¸Ö¼×¨Ö¸× ×Ö±×œÖ¹×”Ö´×™×

[×¡×§×©×Ÿ ×™×¢×“]
×¤×¨×§ ×
×¤×™×¨×•×© ×¢×œ <b>×‘×¨××©×™×ª</b> ×•×¢×œ <b>×‘×¨×</b> ×‘×ª×•×¨×”
×”×¡×‘×¨ × ×•×¡×£ ×›××Ÿ...

[×¤×œ×˜]
×¤×¨×§ ×
×¤×™×¨×•×© ×¢×œ <b>×‘Ö°Ö¼×¨Öµ××©Ö´××™×ª</b> ×•×¢×œ <b>×‘Ö¸Ö¼×¨Ö¸×</b> ×‘×ª×•×¨×”
×”×¡×‘×¨ × ×•×¡×£ ×›××Ÿ..."""
        )
        
        self.chat_session = self.model.start_chat()
        st_log.log("×©×™×¨×•×ª Gemini ××•×›×Ÿ", "âœ…")

    def add_nikud(self, content: Dict, report_path: str = None) -> str:
        """Process content through Gemini to add nikud"""
        st_log.log(f"××¢×‘×“ ×—×œ×§: {content['target_header']}", "ğŸ“")
        
        # Check if target content is too large (more than 100,000 chars)
        target_content = content['target_content']
        if len(target_content) > 100000:
            st_log.log(f"âš ï¸ ×ª×•×›×Ÿ ×”×™×¢×“ ×’×“×•×œ ××“×™ ({len(target_content)} ×ª×•×•×™×), ××¤×¦×œ ×œ×—×œ×§×™× ×§×˜× ×™×", "âš ï¸")
            return self._process_large_content(content, report_path)
        
        prompt = f"""[×˜×§×¡×˜ ××§×•×¨ (×¢× × ×™×§×•×“) - ×”×—×œ×§ ×”×¢×™×§×¨×™]:
{content['source_content']}

[×¡×§×©×Ÿ ×™×¢×“ ××œ× - ×™×© ×œ×©×›×ª×‘ ×‘××“×•×™×§ ×¢× × ×™×§×•×“ ×‘×—×œ×§×™× ×”××•×“×’×©×™× ×‘×œ×‘×“]:
{content['target_content']}

×”× ×—×™×•×ª ×—×©×•×‘×•×ª:
1. ×”×¢×ª×§ ××ª ×›×œ ×”×¡×§×©×Ÿ ×”× "×œ ×‘××“×•×™×§, ××™×œ×” ×‘××™×œ×”, ×›×•×œ×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×”
2. ×”×•×¡×£ × ×™×§×•×“ ××œ× ×œ×›×œ ×˜×§×¡×˜ ×©× ××¦× ×‘×™×Ÿ ×ª×’×™×•×ª <b></b> (×˜×§×¡×˜ ××•×“×’×©)
3. ×•×“× ×©×›×œ ××™×œ×” ××•×“×’×©×ª ××§×‘×œ×ª ××ª ×›×œ ×”× ×™×§×•×“ ×”× ×“×¨×©
4. ×”×©××¨ ××ª ×›×œ ×©××¨ ×”×˜×§×¡×˜ ×‘×“×™×•×§ ×›×¤×™ ×©×”×•×, ×œ×œ× ×©×•× × ×™×§×•×“
5. ×©××•×¨ ×¢×œ ×›×œ ×ª×’×™×•×ª ×”-HTML ×‘××§×•××Ÿ ×”××“×•×™×§
6. ×©××•×¨ ×¢×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×” ×‘××§×•××Ÿ ×”××“×•×™×§ - ×–×” ×§×¨×™×˜×™
7. ××œ ×ª×•×¡×™×£ ×ª×’×™×•×ª <br> ××• ×›×œ ×ª×’×™×ª HTML ××—×¨×ª
8. ××œ ×ª×©× ×” ××ª ×¡×“×¨ ×”××™×œ×™× ××• ×”×ª×•×›×Ÿ
9. ×˜×§×¡×˜ ×©××™× ×• ××•×“×’×© ×—×™×™×‘ ×œ×”×™×©××¨ ×œ×œ× × ×™×§×•×“, ××¤×™×œ×• ×× ×”×•× ×–×”×” ×œ×˜×§×¡×˜ ×× ×•×§×“ ×‘××§×•×¨
10. ×—×¤×© ×‘×˜×§×¡×˜ ×”××§×•×¨ ××ª ×”××™×œ×™× ×”×“×•××•×ª ×‘×™×•×ª×¨ ×œ××™×œ×™× ×”××•×“×’×©×•×ª ×›×“×™ ×œ×”×•×¡×™×£ ×œ×”×Ÿ × ×™×§×•×“ ××ª××™×"""

        # Log full prompt with clear separators
        self.logger.info("\n" + "="*50 + "\nFULL GEMINI PROMPT:\n" + "="*50 + "\n" + prompt)
        
        # Save to report file if provided
        if report_path:
            with open(report_path, 'a', encoding='utf-8') as report_file:
                report_file.write("\n" + "="*50 + "\n")
                report_file.write(f"GEMINI PROMPT FOR SECTION {content['target_header']}:\n")
                report_file.write("="*50 + "\n")
                # Save only a summary of the prompt to avoid extremely large files
                report_file.write(f"[Source content length: {len(content['source_content'])} chars]\n")
                report_file.write(f"[Target content length: {len(content['target_content'])} chars]\n")
                report_file.write("="*50 + "\n")
        
        st_log.log("×©×•×œ×— ×‘×§×©×” ×œ-Gemini...", "ğŸ”„")
        response = self.chat_session.send_message(prompt)
        
        # Log full response with clear separators
        self.logger.info("\n" + "="*50 + "\nFULL GEMINI RESPONSE:\n" + "="*50 + "\n" + response.text)
        
        # Save to report file if provided
        if report_path:
            with open(report_path, 'a', encoding='utf-8') as report_file:
                report_file.write("\n" + "="*50 + "\n")
                report_file.write(f"GEMINI RESPONSE:\n")
                report_file.write("="*50 + "\n")
                # Save only the first 500 characters of the response
                preview = response.text[:500] + "..." if len(response.text) > 500 else response.text
                report_file.write(f"{preview}\n")
                report_file.write("="*50 + "\n\n")
        
        st_log.log(f"×”×ª×§×‘×œ×” ×ª×©×•×‘×” ×-Gemini", "âœ¨")
        
        return response.text
        
    def _process_large_content(self, content: Dict, report_path: str = None) -> str:
        """Process large content by splitting it into chunks and processing each chunk"""
        source_content = content['source_content']
        target_content = content['target_content']
        target_header = content['target_header']
        
        # Find all <b> tags positions to split intelligently
        bold_positions = []
        for match in re.finditer(r'<b>.*?</b>', target_content, re.DOTALL):
            bold_positions.append((match.start(), match.end()))
        
        if not bold_positions:
            st_log.log("×œ× × ××¦××• ×ª×’×™ <b> ×‘×ª×•×›×Ÿ ×”×™×¢×“", "âš ï¸")
            return target_content  # Return unchanged if no bold tags
            
        # Determine chunk size (aim for ~25 bold tags per chunk)
        total_bold_tags = len(bold_positions)
        chunk_count = max(1, total_bold_tags // 25)
        bold_tags_per_chunk = max(1, total_bold_tags // chunk_count)
        
        st_log.log(f"××¤×¦×œ ×œ×›-{chunk_count} ×—×œ×§×™× ×¢× ~{bold_tags_per_chunk} ×ª×’×™ <b> ×‘×›×œ ×—×œ×§", "ğŸ”„")
        
        # Split content into chunks
        chunks = []
        current_chunk_start = 0
        
        for i in range(0, total_bold_tags, bold_tags_per_chunk):
            # If this is the last chunk, include everything to the end
            if i + bold_tags_per_chunk >= total_bold_tags:
                chunk_end = len(target_content)
            else:
                # Otherwise, end the chunk after the last bold tag in this group
                chunk_end = bold_positions[i + bold_tags_per_chunk - 1][1]
            
            # Extract chunk
            chunk = target_content[current_chunk_start:chunk_end]
            chunks.append(chunk)
            current_chunk_start = chunk_end
        
        # Process each chunk
        processed_chunks = []
        
        for idx, chunk in enumerate(chunks):
            st_log.log(f"××¢×‘×“ ×—×œ×§ {idx+1} ××ª×•×š {len(chunks)}", "ğŸ”„")
            
            # Create a fresh chat session for each chunk
            new_session = self.model.start_chat()
            
            # Create chunk content dictionary
            chunk_content = {
                'source_content': source_content,
                'target_content': chunk,
                'target_header': f"{target_header} (×—×œ×§ {idx+1}/{len(chunks)})"
            }
            
            # Construct prompt for this chunk
            prompt = f"""[×˜×§×¡×˜ ××§×•×¨ (×¢× × ×™×§×•×“) - ×”×—×œ×§ ×”×¢×™×§×¨×™]:
{chunk_content['source_content']}

[×—×œ×§ {idx+1} ××ª×•×š {len(chunks)} ×©×œ ×¡×§×©×Ÿ ×™×¢×“ - ×™×© ×œ×©×›×ª×‘ ×‘××“×•×™×§ ×¢× × ×™×§×•×“ ×‘×—×œ×§×™× ×”××•×“×’×©×™× ×‘×œ×‘×“]:
{chunk_content['target_content']}

×”× ×—×™×•×ª ×—×©×•×‘×•×ª:
1. ×”×¢×ª×§ ××ª ×”×—×œ×§ ×”× "×œ ×‘××“×•×™×§, ××™×œ×” ×‘××™×œ×”, ×›×•×œ×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×”
2. ×”×•×¡×£ × ×™×§×•×“ ××œ× ×œ×›×œ ×˜×§×¡×˜ ×©× ××¦× ×‘×™×Ÿ ×ª×’×™×•×ª <b></b> (×˜×§×¡×˜ ××•×“×’×©)
3. ×•×“× ×©×›×œ ××™×œ×” ××•×“×’×©×ª ××§×‘×œ×ª ××ª ×›×œ ×”× ×™×§×•×“ ×”× ×“×¨×©
4. ×”×©××¨ ××ª ×›×œ ×©××¨ ×”×˜×§×¡×˜ ×‘×“×™×•×§ ×›×¤×™ ×©×”×•×, ×œ×œ× ×©×•× × ×™×§×•×“
5. ×©××•×¨ ×¢×œ ×›×œ ×ª×’×™×•×ª ×”-HTML ×‘××§×•××Ÿ ×”××“×•×™×§
6. ×©××•×¨ ×¢×œ ×›×œ ×™×¨×™×“×•×ª ×”×©×•×¨×” ×‘××§×•××Ÿ ×”××“×•×™×§ - ×–×” ×§×¨×™×˜×™
7. ××œ ×ª×•×¡×™×£ ×ª×’×™×•×ª <br> ××• ×›×œ ×ª×’×™×ª HTML ××—×¨×ª
8. ××œ ×ª×©× ×” ××ª ×¡×“×¨ ×”××™×œ×™× ××• ×”×ª×•×›×Ÿ
9. ×˜×§×¡×˜ ×©××™× ×• ××•×“×’×© ×—×™×™×‘ ×œ×”×™×©××¨ ×œ×œ× × ×™×§×•×“, ××¤×™×œ×• ×× ×”×•× ×–×”×” ×œ×˜×§×¡×˜ ×× ×•×§×“ ×‘××§×•×¨
10. ×—×¤×© ×‘×˜×§×¡×˜ ×”××§×•×¨ ××ª ×”××™×œ×™× ×”×“×•××•×ª ×‘×™×•×ª×¨ ×œ××™×œ×™× ×”××•×“×’×©×•×ª ×›×“×™ ×œ×”×•×¡×™×£ ×œ×”×Ÿ × ×™×§×•×“ ××ª××™×"""
            
            # Log chunk info
            if report_path:
                with open(report_path, 'a', encoding='utf-8') as report_file:
                    report_file.write("\n" + "="*50 + "\n")
                    report_file.write(f"CHUNK {idx+1}/{len(chunks)} FOR SECTION {target_header}:\n")
                    report_file.write("="*50 + "\n")
                    report_file.write(f"[Chunk length: {len(chunk)} chars]\n")
                    report_file.write("="*50 + "\n")
            
            # Send chunk to Gemini
            response = new_session.send_message(prompt)
            processed_chunks.append(response.text)
            
        # Combine processed chunks
        st_log.log(f"××©×œ×‘ {len(processed_chunks)} ×—×œ×§×™× ××¢×•×‘×“×™×", "ğŸ”„")
        combined_content = ''.join(processed_chunks)
        
        return combined_content 